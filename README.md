# commander.ai

> **Your personal AI research team.** Delegate complex work to specialized AI agents that think, collaborate, and deliver results - all through natural conversation.

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://www.python.org/)
[![Next.js](https://img.shields.io/badge/Next.js-14-black.svg)](https://nextjs.org/)
[![LangGraph](https://img.shields.io/badge/LangGraph-Latest-green.svg)](https://github.com/langchain-ai/langgraph)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

---

## ğŸ¯ What Makes Commander.ai Different?

Most AI assistants give you one perspective. Commander.ai gives you a **team of specialists** working in parallel, each bringing unique expertise, then intelligently synthesizing their insights into comprehensive answers.

**Think of it as:**
- ğŸ§  A research team that never sleeps
- ğŸ¤ Agents that actually collaborate (not just relay messages)
- ğŸš€ Parallel execution - multiple agents working simultaneously
- ğŸ¯ Intelligent orchestration - the system decides the best approach
- ğŸ“Š Real-time visibility into what's happening

![Commander.ai Architecture](images/architecture.png)

### Memory System: Context That Persists

Every conversation is remembered. Every insight is retained. Commander.ai uses a three-tier memory architecture:

![Memory Architecture](images/memory.png)

- **ğŸ”¥ Hot Layer (Redis)**: Active conversations, instant access
- **ğŸ’¾ Warm Layer (PostgreSQL)**: Complete conversation history
- **ğŸ¯ Smart Layer (Qdrant)**: Semantic search across all agent knowledge

---

## ğŸ”¬ Kai Model Comparison

Commander.ai has been tested across multiple AI model variants to optimize performance and cost. Here's how different configurations perform:

<div align="center">
  <table>
    <tr>
      <td><img src="images/kai-60.jpeg" width="240" alt="Kai 60"></td>
      <td><img src="images/kai-70.jpeg" width="240" alt="Kai 70"></td>
      <td><img src="images/kai-70b.jpeg" width="240" alt="Kai 70B"></td>
      <td><img src="images/kai-complete.jpeg" width="240" alt="Kai Complete"></td>
    </tr>
  </table>
</div>

<p align="center">
  <img src="images/kia-results.png" width="900" alt="Kai Model Results Comparison">
</p>

---

## ğŸ¬ See It In Action

### Real-Time Kanban Board
Watch your tasks flow through the system. See exactly which agent is doing what, when:

![Commander.ai Landing Page](images/landing-page.png?v=2025-02)

### Agent Intelligence Visualized
Ever wondered how AI agents actually think? See their decision trees and execution paths:

![Agent Graph Visualization](images/agent-graph.png)

### Deep Dive into Complex Workflows
Some agents have sophisticated reasoning paths. Zoom in to see how @alice manages her 15-node document workflow:

<p align="center">
  <img src="images/@aliceGraph.png" alt="Full-Screen Graph Viewer - @alice" width="800">
</p>

**Graph Viewer Features:**
- ğŸ” Zoom up to 8x for detailed node inspection
- ğŸ–±ï¸ Click-and-drag panning for navigation
- ğŸ¯ Fit-to-screen button for instant overview
- âŒ¨ï¸ Mouse wheel zoom support
- ğŸ“Š Optimized spacing for complex graphs (10+ nodes)

### Intelligent Agent Selection
Type `@` and get instant autocomplete with agent specializations:

![Agent Autocomplete](images/agent-autocomplete.png)

---

## ğŸ¤– Meet Your AI Team

### Core Orchestration

**ğŸ­ @leo** - *The Orchestrator*
- Uses GPT-4o-mini to analyze your requests
- Decomposes complex tasks into specialized subtasks
- Coordinates parallel agent execution
- Synthesizes results into coherent final answers
- *Think of Leo as your project manager*

### Specialist Agents

**ğŸ”¬ @bob** - *Research Specialist*
- Deep research using Tavily web search + LLM synthesis
- Intelligent content analysis
- Automatic compliance flagging
- Multi-source information synthesis
- *Bob is your investigative journalist*

**âš–ï¸ @sue** - *Compliance Specialist*
- Regulatory compliance analysis
- GDPR, HIPAA, data protection review
- Policy adherence checking
- Risk assessment
- *Sue keeps you out of legal trouble*

**ğŸ“Š @rex** - *Data Analyst*
- Statistical analysis
- Data visualization
- Pattern detection
- Trend analysis
- *Rex turns data into insights*

**ğŸ“š @alice** - *Document Manager*
- PDF and document processing
- Collection management
- Semantic search across documents
- Multi-document analysis
- *Alice is your librarian with superpowers*

### Reasoning Specialists (NEW! ğŸ‰)

**âœ¨ @maya** - *Reflection Specialist*
- Reviews and critiques content
- Identifies issues by severity (critical/important/minor)
- Provides constructive feedback
- Generates improved versions
- Quality scoring (0-1.0)
- *Maya is your editor and quality control*

**ğŸ”„ @kai** - *Reflexion Specialist*
- Self-reflective reasoning with iteration (up to 3 cycles)
- Self-critique and improvement
- Shows reasoning evolution
- Iterative problem solving
- *Kai thinks deeply and improves through reflection*

---

## ğŸš€ Quick Start in 5 Minutes

### Prerequisites

```bash
Python 3.12+, Node.js 18+, Docker (recommended)
```

### Installation

1. **Clone and setup**
   ```bash
   git clone https://github.com/iotlodge/commander.ai.git
   cd commander.ai
   cp .env.example .env
   # Add your OPENAI_API_KEY to .env
   ```

2. **Start infrastructure**
   ```bash
   docker-compose up -d  # PostgreSQL, Redis, Qdrant
   ```

3. **Backend setup**
   ```bash
   uv sync                    # Install dependencies (or: pip install -r requirements.txt)
   alembic upgrade head       # Run migrations
   python -m uvicorn backend.api.main:app --reload
   ```

4. **Frontend setup** (new terminal)
   ```bash
   cd frontend
   npm install
   npm run dev
   ```

5. **Open browser** â†’ http://localhost:3000

**Optional but recommended:** Add `TAVILY_API_KEY` for web search capabilities.

---

## ğŸ’¬ How to Talk to Your AI Team

### Simple Commands

```bash
# Direct to an agent
@bob research the latest quantum computing breakthroughs

# Natural conversation
hey sue, review this privacy policy for GDPR compliance

# Get document help
@alice search for "machine learning" in research_papers
```

### Complex Multi-Agent Workflows

Here's where it gets interesting. Leo (the orchestrator) uses GPT-4o-mini to intelligently decompose your request:

```bash
research "Designing a Quantum Accelerator" investigate ['quantum computing fundamentals', 'accelerator architectures', 'power efficiency', 'cooling systems']
```

**What happens behind the scenes:**

1. ğŸ§  Leo analyzes your request with LLM reasoning
2. ğŸ¯ Decomposes it into focused subtasks
3. ğŸš€ Launches multiple agents **in parallel**
4. ğŸ“Š Each agent investigates their area deeply
5. âœ¨ Leo synthesizes all findings into one coherent report

### Advanced Use Cases

**Content Review & Improvement**
```bash
@maya review this report: [paste your content]

# Maya will:
# - Analyze overall quality
# - Identify issues (critical/important/minor)
# - Suggest specific improvements
# - Generate a refined version
# - Provide a quality score
```

**Self-Reflective Problem Solving**
```bash
@kai solve: How can we optimize database queries for a social media platform with 1M+ concurrent users?

# Kai will:
# - Generate initial reasoning
# - Self-critique the approach
# - Identify flaws in logic
# - Refine reasoning iteratively
# - Show you the evolution of thinking
```

**Document Management**
```bash
# Create a collection
@alice create collection research_papers

# Load documents
@alice load ~/Documents/quantum_paper.pdf into research_papers

# Semantic search
@alice search for "error correction algorithms" in research_papers

# Cross-collection search
@alice search for "quantum gates" in [papers, articles, books]
```

---

## ğŸ“Š Real-Time Execution Metrics & Cost Tracking

Commander.ai provides **complete visibility** into agent execution with live metrics tracking. Monitor LLM usage, token consumption, and execution time in real-time - perfect for optimizing performance and controlling costs.

### Live Metrics During Execution

**What you see:**
- ğŸ”¢ **LLM Calls**: Number of AI model invocations (updates after each call)
- ğŸ¤ **Agent Calls**: Nested agent consultations (tracks delegation depth)
- ğŸ¯ **Tokens**: Total token consumption (prompt + completion)
- â±ï¸ **Duration**: Live execution time in human-friendly format
- ğŸ“ˆ **Progress**: Visual progress bar with current node/step

**Real-time updates mean:**
- Metrics increment **during execution**, not just at completion
- Duration updates **every second** while task is running
- See exactly which iteration @kai is on (e.g., "self_critique_iteration_2")
- Track token usage per LLM call with detailed breakdowns

**Features:**
- ğŸ“‹ **Full Response**: Complete agent output with markdown formatting
- ğŸ“Š **Performance Metrics**: Total LLM calls, agent calls, and token usage
- ğŸ“„ **Metadata**: Execution details, iterations, and quality scores
- ğŸ“‹ **One-Click Copy**: Copy results to clipboard instantly
- ğŸ¨ **Syntax Highlighting**: Code blocks and structured data beautifully formatted

### Token Tracking Examples

**Simple Query (@bob research):**
```
LLM Calls: 3
â”œâ”€ web_search_fallback: 362 tokens
â”œâ”€ research_synthesis: 1,247 tokens
â””â”€ compliance_check: 189 tokens
Total: 1,798 tokens
Duration: 8 seconds
```

**Iterative Reasoning (@kai solve):**
```
LLM Calls: 6 (3 iterations)
â”œâ”€ Iteration 1:
â”‚  â”œâ”€ initial_reasoning: 397 tokens
â”‚  â”œâ”€ self_critique: 655 tokens
â”‚  â””â”€ refine_reasoning: 667 tokens
â”œâ”€ Iteration 2:
â”‚  â”œâ”€ self_critique: 541 tokens
â”‚  â””â”€ refine_reasoning: 856 tokens
â””â”€ Iteration 3:
   â””â”€ self_critique: 855 tokens
Total: 3,971 tokens
Duration: 34 seconds
```

**Multi-Agent Orchestration (@leo):**
```
Agent Calls: 3 (parallel execution)
â”œâ”€ @bob: 1,798 tokens (4 LLM calls)
â”œâ”€ @maya: 2,341 tokens (3 LLM calls)
â””â”€ @rex: 892 tokens (2 LLM calls)
LLM Calls: 11 (9 from agents + 2 aggregation)
Total: 6,247 tokens
Duration: 12 seconds (parallel speedup!)
```

### Cost Optimization Insights

Use metrics to optimize your workflows:

- **Token counts per agent** - Identify expensive operations
- **Execution duration** - Find bottlenecks in your flow
- **Agent call depth** - Optimize delegation strategies
- **LLM call purposes** - See exactly what each call does

**Example: Reducing costs for @kai**
```bash
# Before optimization
@kai solve: complex problem
â†’ 3 iterations, 6 LLM calls, 4,200 tokens

# After setting max_iterations=2
â†’ 2 iterations, 4 LLM calls, 2,800 tokens (33% savings!)
```

---

## ğŸ—ï¸ Architecture That Scales

### Intelligent Task Flow

```
User Query
    â†“
ğŸ­ @leo (Orchestrator)
    â”œâ”€â†’ GPT-4o-mini analyzes request
    â”œâ”€â†’ Decomposes into subtasks
    â””â”€â†’ Assigns to specialists
         â†“
   ğŸš€ Parallel Execution (asyncio.gather)
    â”œâ”€â†’ @bob (Research)
    â”œâ”€â†’ @maya (Reflection)
    â”œâ”€â†’ @kai (Reflexion)
    â””â”€â†’ [@sue, @rex, @alice...]
         â†“
   âœ¨ LLM-Powered Aggregation
    â”œâ”€â†’ Synthesizes all findings
    â”œâ”€â†’ Executive summary
    â”œâ”€â†’ Key insights from each specialist
    â””â”€â†’ Actionable recommendations
         â†“
   ğŸ“„ Final Response (Markdown)
```

### What Makes This Fast?

**Parallel Execution**: When Leo determines you need research from multiple areas, all agents run simultaneously using `asyncio.gather()`. A 3-agent task completes in ~10 seconds instead of ~30 seconds sequential.

**Smart Caching**: Redis keeps recent conversations hot. PostgreSQL stores everything. Qdrant enables semantic search across millions of data points.

**LLM Optimization**: Uses GPT-4o-mini for reasoning tasks (fast + cost-effective). Automatically falls back to pattern matching if needed.

---

## ğŸ› ï¸ Tech Stack

### Backend (Python)
- **LangGraph** - Agent orchestration framework
- **LangChain** - LLM integration layer
- **FastAPI** - Async web framework
- **PostgreSQL** - Persistent storage
- **Redis** - Hot memory layer (active sessions)
- **Qdrant** - Vector database for semantic search
- **OpenAI GPT-4o-mini** - Intelligence layer

### Frontend (TypeScript)
- **Next.js 14** - React with App Router
- **Tailwind CSS** - Utility-first styling
- **shadcn/ui** - Beautiful component library
- **Zustand** - State management
- **WebSocket** - Real-time updates

### Infrastructure
- **Docker Compose** - One-command setup
- **Alembic** - Database migrations
- **Async Python** - Non-blocking I/O

---

## ğŸ“Š Project Structure

```
commander.ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base/           # Agent interface & registry
â”‚   â”‚   â”œâ”€â”€ parent_agent/   # @leo orchestrator
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_reasoning.py      # GPT-4o-mini decomposition
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_aggregation.py    # Result synthesis
â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py              # Graph nodes (parallel exec)
â”‚   â”‚   â”‚   â””â”€â”€ graph.py              # LangGraph workflow
â”‚   â”‚   â””â”€â”€ specialized/
â”‚   â”‚       â”œâ”€â”€ agent_a/    # @bob (Research) - LLM-powered
â”‚   â”‚       â”œâ”€â”€ agent_b/    # @sue (Compliance)
â”‚   â”‚       â”œâ”€â”€ agent_c/    # @rex (Data Analysis)
â”‚   â”‚       â”œâ”€â”€ agent_d/    # @alice (Documents)
â”‚   â”‚       â”œâ”€â”€ agent_e/    # @maya (Reflection) âš¡ NEW
â”‚   â”‚       â””â”€â”€ agent_f/    # @kai (Reflexion) âš¡ NEW
â”‚   â”œâ”€â”€ memory/             # Three-tier memory system
â”‚   â”‚   â”œâ”€â”€ short_term.py   # Redis (hot layer)
â”‚   â”‚   â”œâ”€â”€ long_term.py    # PostgreSQL (warm layer)
â”‚   â”‚   â”œâ”€â”€ vector_store.py # Qdrant (semantic layer)
â”‚   â”‚   â””â”€â”€ memory_service.py
â”‚   â”œâ”€â”€ api/                # FastAPI app + WebSocket
â”‚   â””â”€â”€ core/               # Config, parsers, executors
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/                # Next.js 14 App Router
â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”‚   â”œâ”€â”€ kanban/        # Real-time task board
â”‚   â”‚   â”œâ”€â”€ graphs/        # Agent visualization
â”‚   â”‚   â””â”€â”€ command/       # Input with autocomplete
â”‚   â””â”€â”€ lib/               # WebSocket, state, types
â””â”€â”€ migrations/            # Database schema versions
```

---

## âš™ï¸ Configuration

### Required Environment Variables

```bash
# Core - REQUIRED
OPENAI_API_KEY=sk-...                # GPT-4o-mini for reasoning

# Database - Auto-configured by Docker
DATABASE_URL=postgresql+asyncpg://commander:changeme@localhost:5432/commander_ai
REDIS_URL=redis://localhost:6379/0
QDRANT_URL=http://localhost:6333

# Optional - Enhances Capabilities
TAVILY_API_KEY=tvly-...              # Web search for @bob
ANTHROPIC_API_KEY=sk-ant-...         # Alternative LLM provider
```

### Docker Services (Included)

```bash
docker-compose up -d
```

This starts:
- **PostgreSQL 16** (pgvector extension)
- **Redis 7** (Alpine - lightweight)
- **Qdrant** (Latest vector DB)

All services have health checks and auto-restart.

---

## ğŸ’¡ Real-World Examples

### Example 1: Market Research

**Command:**
```bash
research "AI Agent Frameworks 2025" investigate ['current market leaders', 'emerging technologies', 'adoption trends', 'cost comparison']
```

**What you get:**
- ğŸ”¬ @bob researches each area in parallel
- ğŸ“Š Comprehensive findings from multiple sources
- ğŸ¯ Executive summary with key insights
- ğŸ’¡ Recommendations based on trends
- â±ï¸ Complete in ~10 seconds (vs 40s sequential)

### Example 2: Content Quality Control

**Command:**
```bash
@maya review this article: [paste 2000-word article]
```

**What you get:**
- ğŸ“ Overall quality assessment (0-1.0 score)
- ğŸ”´ Critical issues that must be fixed
- ğŸŸ¡ Important improvements
- ğŸŸ¢ Minor polish suggestions
- âœ¨ Refined version with improvements applied

### Example 3: Deep Problem Solving

**Command:**
```bash
@kai analyze: Why are our API response times degrading under load?
```

**What you get:**
- ğŸ§  Initial hypothesis and reasoning
- ğŸ” Self-critique of the analysis
- ğŸ”„ Refined reasoning (iteration 2)
- ğŸ” Another self-critique
- âœ… Final comprehensive analysis (iteration 3)
- ğŸ“ˆ Shows evolution of thinking

### Example 4: Document Intelligence

**Command:**
```bash
@alice load research_papers/*.pdf into quantum_research
@alice search for "topological qubits" in quantum_research
@alice summarize findings about error correction
```

**What you get:**
- ğŸ“š All PDFs indexed with vector embeddings
- ğŸ¯ Semantic search (not just keywords)
- ğŸ“Š Cross-document synthesis
- ğŸ’¡ Connections you might have missed

---

## ğŸ® Keyboard Shortcuts

- `Cmd/Ctrl + Enter` - Submit command
- `@` - Trigger agent autocomplete
- `Esc` - Close modals/dialogs

---

## ğŸš¦ Current Status

**âœ… Production Ready (v1.0)**
- âœ… LLM-powered task decomposition (GPT-4o-mini)
- âœ… Parallel multi-agent execution (asyncio.gather)
- âœ… Intelligent result aggregation
- âœ… 7 specialized agents (including @maya and @kai)
- âœ… Three-tier memory system (Redis/PostgreSQL/Qdrant)
- âœ… Real-time Kanban UI with WebSocket
- âœ… Agent graph visualization
- âœ… Full-screen graph viewer with zoom/pan
- âœ… Document management with semantic search

**ğŸš§ In Active Development**
- â³ User authentication & multi-user support
- â³ CLI interface for terminal lovers
- â³ Export to .docx/.pdf (currently markdown)
- â³ Agent performance metrics dashboard
- â³ Custom agent creation wizard

**ğŸ“… Roadmap**
- Vision/image analysis agents
- Code execution agents (sandboxed)
- Plugin system for custom tools
- Agent marketplace
- Enterprise SSO integration

---

## ğŸ¤ Contributing

This started as a personal project to explore what's possible when you give AI agents the ability to truly collaborate. If you're excited about multi-agent systems, LangGraph, or just want to make AI more useful, contributions are welcome!

**Ways to contribute:**
- ğŸ› Report bugs or UX issues
- ğŸ’¡ Suggest new agent specializations
- ğŸ“ Improve documentation or examples
- ğŸ§ª Add test coverage
- âš¡ Performance optimizations

**Before submitting a PR:**
1. Run tests: `pytest`
2. Check linting: `ruff check .`
3. Ensure type safety: `mypy backend/`

---

## ğŸ“œ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

**Key Points:**
- âœ… Commercial use allowed
- âœ… Modification allowed
- âœ… Distribution allowed
- âœ… Patent use allowed
- âš–ï¸ License and copyright notice required
- âš–ï¸ State changes required

---

## ğŸ™ Acknowledgments

Built on the shoulders of giants:
- **[LangGraph](https://github.com/langchain-ai/langgraph)** - The foundation for agent orchestration
- **[LangChain](https://github.com/langchain-ai/langchain)** - LLM integration made simple
- **[shadcn/ui](https://ui.shadcn.com/)** - Beautiful, accessible components
- **[Lucide Icons](https://lucide.dev/)** - Clean, consistent icons
- **OpenAI** - GPT-4o-mini powers the intelligence layer

---

## ğŸ“š Further Reading

- [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md) - Deep dive into the LLM system
- [Commander_ai_Project_Plan.md](Commander_ai_Project_Plan.md) - Original vision and requirements
- [PLAN.md](PLAN.md) - Detailed development roadmap

---

## ğŸ’¬ Final Thoughts

Commander.ai isn't just another AI assistant. It's a glimpse into how we'll work with AI in the future - not as a single tool, but as a **team of specialists** that collaborate, reason, and deliver results that are greater than the sum of their parts.

The magic happens when:
- @bob finds information you didn't know to look for
- @maya catches issues before they become problems
- @kai reasons through complex problems iteratively
- @leo brings it all together into actionable insights

**Try it yourself.** Watch the agents work. See the reasoning unfold. You'll never go back to single-agent assistants.

---

**Questions? Issues? Ideas?**
ğŸ“§ Open an issue or discussion on GitHub
â­ Star the repo if this excites you
ğŸ”” Watch for updates - this is moving fast

---

*Built with â¤ï¸ by developers who believe AI should augment human capability, not replace it.*

---

**Status**: ğŸš€ v1.0 - Production Ready
**Last Updated**: February 2025
